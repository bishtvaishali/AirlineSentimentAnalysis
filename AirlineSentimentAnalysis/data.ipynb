{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, csv\n",
    "\n",
    "\n",
    "csvfile = \"Tweets.csv\"\n",
    "balancedfile = \"BalancedUncleanFile.csv\"\n",
    "\n",
    "def createBalancedFile(): \n",
    "    csvfile = \"Tweets.csv\"\n",
    "#     balancedfile = \"BalancedUncleanFile.csv\"\n",
    "    df = pd.read_csv(csvfile)#Pandas DataFrame is 2-D,tabular data structure with labeled rows and columns\n",
    "#     df.info()s\n",
    "\n",
    "    #counting total no. of records\n",
    "    totalPositiveRecords = len(df[df[\"airline_sentiment\"] == \"positive\"])\n",
    "    totalNegativeRecords = len(df[df[\"airline_sentiment\"] == \"negative\"])\n",
    "    totalNeutralRecords = len(df[df[\"airline_sentiment\"] == \"neutral\"])\n",
    "    print(\"Unbalanced Sentiment Values:\",df.airline_sentiment.value_counts())\n",
    "#     print(\"COLUMN NAMEs:\", df.columns)\n",
    "    print(\"AIRLINES\", df.airline.unique())\n",
    "    df.airline_sentiment.value_counts().plot(kind='bar')\n",
    "\n",
    "    #indices\n",
    "    positiveIndices = df[df['airline_sentiment'] == \"positive\"].index\n",
    "    negativeIndices = df[df['airline_sentiment'] == \"negative\"].index\n",
    "    neutralIndices = df[df['airline_sentiment'] == \"neutral\"].index\n",
    "\n",
    "    #randomaly selecting the records wrt sentiments\n",
    "    random_Positive_indices = np.random.choice(positiveIndices, totalPositiveRecords, replace=False)\n",
    "    random_Negative_indices = np.random.choice(negativeIndices, totalPositiveRecords, replace=False)\n",
    "    random_Neutral_indices = np.random.choice(neutralIndices, totalPositiveRecords, replace=False)\n",
    "    \n",
    "    print(\"RANDOM VALUES: Positive = \" ,len(random_Positive_indices),\"Negative=\"  ,len(random_Negative_indices),\\\n",
    "          \"Neutal=\", len(random_Neutral_indices))\n",
    "    allIndices = np.concatenate([random_Positive_indices,random_Negative_indices,random_Neutral_indices])\n",
    "    df = df.iloc[allIndices] \n",
    "    df.to_csv(balancedfile) #converting back to csv\n",
    "    print(\"A balanced *\",balancedfile,\"* file created\")\n",
    "    \n",
    "   \n",
    "    balancedDf = pd.read_csv(balancedfile)\n",
    "    print(\"sentimentValues:\",df.airline_sentiment.value_counts())\n",
    "    balancedDf.airline_sentiment.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleaning():\n",
    "    from nltk.corpus import stopwords \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re, csv\n",
    "    \n",
    "    balancedfile = \"BalancedUncleanFile.csv\"\n",
    "    stopWordList = set(stopwords.words(\"english\")) \n",
    "    stemmer = PorterStemmer()\n",
    "    punctuations = \"-?.!\"\n",
    "\n",
    "    tweetsData = []\n",
    "    sentiment = []\n",
    "    dict1 = {}\n",
    "    \n",
    "\n",
    "    with open(balancedfile) as csvfile:\n",
    "        data = csv.reader(csvfile, delimiter=',')\n",
    "        i=0\n",
    "        for row in data:\n",
    "            data = row[13]\n",
    "            senti = row[4]\n",
    "            sentiment.append(senti)\n",
    "            tokenArr = re.split('[(\\[\\]\\+*:.!\\,-?;)]', data)\n",
    "            cleanTextList = []        \n",
    "            for token in tokenArr:\n",
    "                if not token in stopWordList:\n",
    "                    token = stemmer.stem(token) #stemming\n",
    "                    if token in punctuations:#replacing punctuations\n",
    "                        token = \"<PUNCT>\"\n",
    "                    else:\n",
    "                        #token = re.sub('#[^\\s]+',\"<HASHTAG>\",token)#replacing #hastage \n",
    "                        token = re.sub('@[^\\s]+',\"<USERNAME>\",token)#replacing @username from token \n",
    "                        token = re.sub(r\"http\\S+\", \"<URL>\", token)  #replacing url from token     \n",
    "                        token = re.sub(r\"\\//\\S+\", \"<URL>\", token)  #replacing url from token like this: //t.co/mWpG7grEZP\n",
    "                    token = token.lower()\n",
    "\n",
    "                cleanTextList.append(token)\n",
    "\n",
    "            cleanText = ' '.join(cleanTextList)\n",
    "            tweetsData.append(cleanText)\n",
    "    \n",
    "    array = [tweetsData, sentiment]\n",
    "#     print(tweetsData)\n",
    "    import re, csv\n",
    "    wtr = csv.writer(open ('balanced_clean.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "    \n",
    "    for i in range(1,7060) : \n",
    "        wtr.writerow ([sentiment[i], tweetsData[i]])\n",
    "\n",
    "        \n",
    "    print(\"EOD\")\n",
    "    return array\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCleaning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCleanDataToFile():\n",
    "    import re, csv\n",
    "    wtr = csv.writer(open ('balanced_clean.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "    \n",
    "    for i in range(1,7060) : \n",
    "        wtr.writerow ([sentiment[i], tweetsData[i]])\n",
    "    \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createBalancedFile()\n",
    "dataCleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def POS():\n",
    "#     import nltk\n",
    "#     nltk.download('averaged_perceptron_tagger')\n",
    "#     balancedfile = \"balanced_clean.csv\"\n",
    "\n",
    "#     tweetsData = []\n",
    "#     sentiment = []\n",
    "#     print(\"running...\")\n",
    "    \n",
    "#     with open(balancedfile) as csvfile:\n",
    "#         data = csv.reader(csvfile, delimiter=',')\n",
    "#         print(data)\n",
    "#         for row in data:\n",
    "#             data = row[1]\n",
    "#             senti = row[0]\n",
    "          \n",
    "#             sentiment.append(senti)\n",
    "#             tokenArr = nltk.word_tokenize(data)\n",
    "# #             print(token)\n",
    "#             pos = nltk.pos_tag(tokenArr)\n",
    "#             print(pos)\n",
    "#             tweetsData.append(pos)\n",
    "\n",
    "    \n",
    "#     array = [tweetsData, sentiment]\n",
    "#     print(tweetsData[:10])\n",
    "#     print(\"EOD\")\n",
    "    \n",
    "#     print(\"done\")\n",
    "    \n",
    "    \n",
    "#     return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PosArray = dataCleaning1()\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectoriser = DictVectorizer(sparse=False)\n",
    "# X = vectoriser.fit_transform(PosArray[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(\"The Beatles good\")\n",
    "# tokens\n",
    "\n",
    "PosArray = nltk.pos_tag(tokens)\n",
    "# X = vectoriser.fit_transform(PosArray)\n",
    "\n",
    "PosArray[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(\"band formed in\")\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
