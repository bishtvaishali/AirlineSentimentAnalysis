{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE: (279, 1243)\n",
      "accuracy: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.00      0.00      0.00        46\n",
      "    negative       0.00      0.00      0.00         0\n",
      "     neutral       0.00      0.00      0.00        94\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       140\n",
      "   macro avg       0.00      0.00      0.00       140\n",
      "weighted avg       0.00      0.00      0.00       140\n",
      "\n",
      "SHAPE: (279, 1192)\n",
      "accuracy: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.00      0.00      0.00        27\n",
      "    negative       0.00      0.00      0.00        86\n",
      "     neutral       0.00      0.00      0.00        27\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       140\n",
      "   macro avg       0.00      0.00      0.00       140\n",
      "weighted avg       0.00      0.00      0.00       140\n",
      "\n",
      "SHAPE: (280, 1242)\n",
      "accuracy: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.00      0.00      0.00       139\n",
      "    negative       0.00      0.00      0.00         0\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       139\n",
      "   macro avg       0.00      0.00      0.00       139\n",
      "weighted avg       0.00      0.00      0.00       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------TRAINING & TESTING DATASET CREATION--------------\n",
    "import xlrd, re\n",
    "\n",
    "\n",
    "path = (\"Tweets.xls\") \n",
    "spreadSheet = xlrd.open_workbook(path)\n",
    "\n",
    "tweetsData = []\n",
    "sentiment = [] \n",
    "targetSentiment = [\"positive\", \"negative\",\"neutral\"]\n",
    "\n",
    "sheet = spreadSheet.sheet_by_index(1) \n",
    "for i in range(1, 420):\n",
    "    arrStr = re.sub('[!@#\\\"$]', \"\", sheet.cell_value(i, 10)) \n",
    "    arrStr = re.sub(r\"\\//\\S+\", \"\", arrStr)\n",
    "    tweetsData.append(arrStr) \n",
    "    sentiment.append(sheet.cell_value(i, 1))\n",
    "    \n",
    "\n",
    "def calScore(X_train, X_test, Y_train, Y_test):\n",
    "    #--------------MODEL CREATION---------------\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "\n",
    "    countVector = CountVectorizer()\n",
    "    vocab= countVector.fit(X_train)\n",
    "    # print(\"\\nVocabulary: \" + str(countVector.vocabulary_) + '\\n')\n",
    "    # print(\"Features: \" + str(countVector.get_feature_names()) + '\\n') \n",
    "\n",
    "    termCountMatrix = countVector.transform(X_train)\n",
    "    print(\"SHAPE: \"+ str(termCountMatrix.shape)) #3 documents and 7 unique terms/vocab\n",
    "    # print(\"COUNT MATRIX: \\n\"+ str(termCountMatrix.toarray())) \n",
    "\n",
    "    #terms given tfIdf\n",
    "    tfIdfTransformer = TfidfTransformer()\n",
    "    tfIdfTransformer.fit(termCountMatrix)#sorted in the order of Features array\n",
    "    # print(\"TFIDF:\" + str(tfIdfTransformer.idf_))\n",
    "\n",
    "    tdIdfMatrix = tfIdfTransformer.transform(termCountMatrix)\n",
    "    # print(\"TFIDF:\" + str(tdIdfMatrix.toarray()))\n",
    "\n",
    "    model = MultinomialNB().fit(tdIdfMatrix,Y_train)\n",
    "    # model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "    # model.fit(trainDict.values(), trainDict.keys())\n",
    "    # labels = model.predict(testDict.values())\n",
    "\n",
    "    #-------------------TESTING-----------------------\n",
    "    newVectorMatrix = countVector.transform(X_test)\n",
    "    newTFMatrix =  tfIdfTransformer.transform(newVectorMatrix)\n",
    "    predicted = model.predict(newTFMatrix)\n",
    "#     print(\"PREDICT\",predicted)\n",
    "\n",
    "    #TEST RESULTS: \n",
    "    #without stop word removal\n",
    "    # test_size =0.3 ---> accuracy: 0.677595, 0.6673497, 0.66074, 0.67668\n",
    "    # test_size =0.2 ---> 0.659153, 0.6639344, 0.67657,0.67725409, 0.6851092, 0.6803278, 0.691598\n",
    "    # test_size =0.4 ---> 0.6622267759562842, 0.67400, 0.6816939890710383, 0.6798\n",
    "    # test_size =0.5 ---> 0.6643442622950819, 0.671, 0.6653005, 0.6569\n",
    "    # test_size =0.8 ---> 0.6534323770491803\n",
    "\n",
    "    from sklearn import metrics\n",
    "\n",
    "    accuracy = metrics.accuracy_score(Y_test,predicted)\n",
    "    print(\"accuracy:\", accuracy)\n",
    "    print(metrics.classification_report(Y_test, predicted, targetSentiment))\n",
    "    metrics.confusion_matrix(Y_test,predicted)\n",
    "    \n",
    " \n",
    "    # DATA SPLIT\n",
    "    # from sklearn.model_selection import train_test_split\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(tweetsData, sentiment, test_size =0.2)\n",
    "    # calScore(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "\n",
    "\n",
    "# ---------KFold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "kf  = KFold(n_splits=3)\n",
    "kf\n",
    "# for train_index, test_index in kf.split([1,2,3,4,5,6,7,8,9]):\n",
    "#     print(train_index, test_index)\n",
    "     \n",
    "for train_index, test_index in kf.split(tweetsData):   \n",
    "    X_train = np.array(tweetsData)[train_index]\n",
    "    Y_train = np.array(tweetsData)[train_index]\n",
    "    X_test = np.array(sentiment)[test_index]\n",
    "    Y_test = np.array(sentiment)[test_index]\n",
    "#     i=1\n",
    "#     print(\"KFOLD:\", i, \": \", X_test)\n",
    "    calScore(X_train, X_test, Y_train, Y_test)    \n",
    "#     i=i+1\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
