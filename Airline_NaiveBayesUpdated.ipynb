{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      "tweet_id                        14640 non-null int64\n",
      "airline_sentiment               14640 non-null object\n",
      "airline_sentiment_confidence    14640 non-null float64\n",
      "negativereason                  9178 non-null object\n",
      "negativereason_confidence       10522 non-null float64\n",
      "airline                         14640 non-null object\n",
      "airline_sentiment_gold          40 non-null object\n",
      "name                            14640 non-null object\n",
      "negativereason_gold             32 non-null object\n",
      "retweet_count                   14640 non-null int64\n",
      "text                            14640 non-null object\n",
      "tweet_coord                     1019 non-null object\n",
      "tweet_created                   14640 non-null object\n",
      "tweet_location                  9907 non-null object\n",
      "user_timezone                   9820 non-null object\n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.7+ MB\n",
      "No of Positive=  2363\n",
      "No of Negative=  9178\n",
      "No of Neutral =  3099\n",
      "A balanced * BalancedUncleanFile.csv * file created\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "csvfile = \"Tweets.csv\"\n",
    "balancedfile = \"BalancedUncleanFile.csv\"\n",
    "df = pd.read_csv(csvfile)#Pandas DataFrame is 2-D,tabular data structure with labeled rows and columns\n",
    "df.info()\n",
    "\n",
    "#counting total no. of records\n",
    "totalPositiveRecords = len(df[df[\"airline_sentiment\"] == \"positive\"])\n",
    "totalNegativeRecords = len(df[df[\"airline_sentiment\"] == \"negative\"])\n",
    "totalNeutralRecords = len(df[df[\"airline_sentiment\"] == \"neutral\"])\n",
    "print(\"No of Positive= \", totalPositiveRecords) #2363\n",
    "print(\"No of Negative= \",totalNegativeRecords)\n",
    "print(\"No of Neutral = \", totalNeutralRecords)\n",
    "\n",
    "#indices\n",
    "negativeIndices = df[df['airline_sentiment'] == \"negative\"].index\n",
    "positiveIndices = df[df['airline_sentiment'] == \"positive\"].index\n",
    "neutralIndices = df[df['airline_sentiment'] == \"neutral\"].index\n",
    "\n",
    "#randomaly selecting the records wrt sentiments\n",
    "random_Positive_indices = np.random.choice(positiveIndices, totalPositiveRecords, replace=False)\n",
    "random_Negative_indices = np.random.choice(negativeIndices, totalPositiveRecords, replace=False)\n",
    "random_Neutral_indices = np.random.choice(neutralIndices, totalPositiveRecords, replace=False)\n",
    "\n",
    "allIndices = np.concatenate([random_Positive_indices,random_Negative_indices,random_Neutral_indices])\n",
    "df = df.iloc[allIndices] \n",
    "df.to_csv(balancedfile) #converting back to csv\n",
    "print(\"A balanced *\",balancedfile,\"* file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOD\n"
     ]
    }
   ],
   "source": [
    "import re, csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "stopWordList = set(stopwords.words('english')) \n",
    "stemmer = PorterStemmer()\n",
    "punctuations = \"-?.!'\"\n",
    "\n",
    "tweetsData = []\n",
    "sentiment = []\n",
    "dict1 = {}\n",
    " \n",
    "with open(balancedfile) as csvfile:\n",
    "    data = csv.reader(csvfile, delimiter=',')\n",
    "    i=0\n",
    "    for row in data:\n",
    "        data = row[11]\n",
    "        senti = row[2]\n",
    "        sentiment.append(senti)\n",
    "        tokenArr = re.split('[(\\[\\]\\+*:.!\\,-?;)]', data)\n",
    "#         tokenArr =  data.split()\n",
    "        cleanTextList = []        \n",
    "        for token in tokenArr:\n",
    "            if not token in stopWordList:\n",
    "                token = stemmer.stem(token) #stemming\n",
    "                if token in punctuations:#replacing punctuations\n",
    "                #print(token)\n",
    "                    token = \"<PUNCT>\"\n",
    "                else:\n",
    "                    #token = re.sub('#[^\\s]+',\"<HASHTAG>\",token)#replacing #hastage \n",
    "                    token = re.sub('@[^\\s]+',\"<USERNAME>\",token)#replacing @username from token \n",
    "                    token = re.sub(r\"http\\S+\", \"<URL>\", token)  #replacing url from token     \n",
    "                    token = re.sub(r\"\\//\\S+\", \"<URL>\", token)  #replacing url from token like this: //t.co/mWpG7grEZP\n",
    "                token = token.lower()\n",
    "\n",
    "            cleanTextList.append(token)\n",
    "            \n",
    "        cleanText = ' '.join(cleanTextList)\n",
    "        tweetsData.append(cleanText)\n",
    "#         if(i >= 30):\n",
    "#             break\n",
    "#         i=1+1\n",
    "\n",
    "    \n",
    "\n",
    "# for i in tweetsData: print (i)\n",
    "# print(\"balanced_clean created\")     \n",
    "# print('TWEETS:', tweetsData)\n",
    "print(\"EOD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wtr = csv.writer(open ('balanced_clean.csv', 'w'), delimiter=',', lineterminator='\\n')\n",
    "for i in range(1,7068) : \n",
    "    wtr.writerow ([sentiment[i], tweetsData[i]])\n",
    "    \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "SHAPE:(4608, 7243)\n",
      "PREDICT ['positive' 'positive' 'neutral' ... 'neutral' 'negative' 'neutral']\n",
      "accuracy: 0.7103142626913779\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.79      0.72      0.75       865\n",
      "    negative       0.66      0.83      0.73       814\n",
      "     neutral       0.70      0.58      0.63       803\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      2482\n",
      "   macro avg       0.72      0.71      0.71      2482\n",
      "weighted avg       0.72      0.71      0.71      2482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Splitting DataSet for Training and Testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "targetSentiment = [\"positive\", \"negative\",\"neutral\"]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(tweetsData, sentiment, test_size =0.35)\n",
    "#Convert a collection of text documents to a matrix of token counts\n",
    "#Count Vectorizer ignores uni-character words as it has no information\n",
    "countVector = CountVectorizer() \n",
    "vocab= countVector.fit(X_train)\n",
    "termCountMatrix = countVector.transform(X_train)\n",
    "print(\"running\")\n",
    "print(\"SHAPE:\"+ str(termCountMatrix.shape)) #3 documents and 7 unique terms/vocab\n",
    "\n",
    "#TFIDF\n",
    "tfIdfTransformer = TfidfTransformer()\n",
    "tfIdfTransformer.fit(termCountMatrix)#sorted in the order of Features array\n",
    "#print(\"TFIDF:\" + str(tfIdfTransformer.idf_))\n",
    "\n",
    "tdIdfMatrix = tfIdfTransformer.transform(termCountMatrix)\n",
    "\n",
    "\n",
    "# print(\"TFIDF:\" + str(tdIdfMatrix.toarray()))\n",
    "\n",
    "\n",
    "#------Training NB Model-----\n",
    "model = MultinomialNB().fit(tdIdfMatrix,Y_train)\n",
    "\n",
    "\n",
    "#------Testing NB Model-----\n",
    "newVectorMatrix = countVector.transform(X_test)\n",
    "newTFMatrix =  tfIdfTransformer.transform(newVectorMatrix)\n",
    "predicted = model.predict(newTFMatrix)\n",
    "print(\"PREDICT\",predicted)\n",
    "\n",
    "accuracy = metrics.accuracy_score(Y_test,predicted)\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(metrics.classification_report(Y_test, predicted, targetSentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# vectorizer = TfidfVectorizer(min_df =2, max_df=0.5, ngram_range=(1,2))\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "# vectorizer = TfidfVectorizer(sublinear_tf=True, \n",
    "#                              smooth_idf = True,\n",
    "#                              stop_words='english')\n",
    "\n",
    "features = vectorizer.fit_transform(X_train)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "# print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeatures = vectorizer.transform(X_test)\n",
    "# print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE: feature:(4608, 7243)featureTest:(2482, 7243)\n",
      "SELECT K BEST, feature:(4608, 1000)featureTest:(2482, 1000)\n",
      "PREDICT ['positive' 'positive' 'neutral' ... 'neutral' 'negative' 'neutral']\n",
      "accuracy: 0.7159548751007252\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.80      0.74      0.77       865\n",
      "    negative       0.70      0.76      0.73       814\n",
      "     neutral       0.65      0.64      0.65       803\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      2482\n",
      "   macro avg       0.72      0.72      0.71      2482\n",
      "weighted avg       0.72      0.72      0.72      2482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "print(\"SHAPE: feature:\"+ str(features.shape) + \"featureTest:\" + str(testFeatures.shape))\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "ch2 = SelectKBest(chi2, k=1000)\n",
    "X_train = ch2.fit_transform(features, Y_train)\n",
    "X_test = ch2.transform(testFeatures);\n",
    "selected_feature_names = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "# print(selected_feature_names);\n",
    "\n",
    "print(\"SELECT K BEST, feature:\"+ str(X_train.shape) + \"featureTest:\" + str(X_test.shape))\n",
    "\n",
    "#------Training NB Model-----\n",
    "model = MultinomialNB().fit(X_train,Y_train)\n",
    "\n",
    "#------Testing NB Model-----\n",
    "predicted = model.predict(X_test)\n",
    "print(\"PREDICT\",predicted)\n",
    "\n",
    "accuracy = metrics.accuracy_score(Y_test,predicted)\n",
    "print(\"accuracy:\", accuracy)\n",
    "print(metrics.classification_report(Y_test, predicted, targetSentiment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[576 104 185]\n",
      " [ 57 579 178]\n",
      " [ 92 166 545]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(47.25, 0.5, 'predicted label')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConMetrics = metrics.confusion_matrix(Y_test,predicted,targetSentiment)\n",
    "print(ConMetrics)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "sns.heatmap(ConMetrics.T, annot=True, fmt='d', xticklabels= targetSentiment, yticklabels= targetSentiment)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-928f5ff6511b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnewVectorMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcountVector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnewTFMatrix\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtfIdfTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewVectorMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewTFMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PREDICT\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         return (safe_sparse_dot(X, self.feature_log_prob_.T) +\n\u001b[0m\u001b[1;32m    732\u001b[0m                 self.class_log_prior_)\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \"\"\"\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "test =  ['bad', 'sad', 'happy', 'unhappy']\n",
    "newVectorMatrix = countVector.transform(test)\n",
    "newTFMatrix =  tfIdfTransformer.transform(newVectorMatrix)\n",
    "predicted = model.predict(newTFMatrix)\n",
    "print(\"PREDICT\",predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
